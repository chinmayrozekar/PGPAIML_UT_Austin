{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "TiYIU8lPFGwt",
        "daBFxPKyAhrl",
        "BS5B2nU0GhzL",
        "f85iiAH66Yog",
        "STDSb04iT-rL",
        "JWD7rPCRUEtD",
        "UsCYxkq_UL3Q",
        "beq1RbMhUQmi",
        "dPUTD0Snew7n",
        "3mU64zPDeyKR"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Problem Statement"
      ],
      "metadata": {
        "id": "TiYIU8lPFGwt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Business Context"
      ],
      "metadata": {
        "id": "daBFxPKyAhrl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the competitive landscape of retail banking, customer retention is critical for ensuring sustainable growth and profitability. A prominent retail banking institution in Europe provides a range of financial products, including credit cards, loans, and savings accounts, and has been rapidly expanding its customer base across multiple countries. However, with a growing customer base, it faces an increasingly pressing challenge: customer churn. A significant number of customers are closing their accounts and switching to competitors. This decline in customer retention is impacting revenue and long-term customer relationships\n",
        "\n",
        "Understanding the reasons behind customer attrition (or churn) is essential for the bank to devise effective retention strategies to minimize churn and enhance customer loyalty and satisfaction. The Customer Analytics & Retention Department has been diligently collecting and analyzing historical customer data. Despite the valuable insights provided by historical data, the department grapples with several challenges:\n",
        "\n",
        "1. **Complex Customer Behavior**: The diverse nature of the bank's offerings and the varying customer preferences across different countries complicate the identification of factors that lead to churn.\n",
        "2. **Proactive Retention**: The current processes for identifying at-risk customers are reactive rather than proactive, leading to missed opportunities for timely interventions that could prevent churn."
      ],
      "metadata": {
        "id": "ucRv3UZ6zed-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objective"
      ],
      "metadata": {
        "id": "BS5B2nU0GhzL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Customer Analytics & Retention Department deployed a churn prediction model as a simple web application to enable real-time, data-driven decision-making through an intuitive interface. However, as access was scaled to geographically distributed teams, the centralized deployment model led to performance bottlenecks and increased latency. Attempts to share the deployment model resulted in frequent failures due to inconsistencies in system environments, operating systems, and dependencies.\n",
        "\n",
        "To address these challenges, the department aims to find a better mechanism to package the model, environment, dependencies, and configuration into a standardized unit that runs reliably across different systems. The objective is to eliminate compatibility issues, reduce deployment errors, simplify distribution, and allow each location to use the app with minimal setup. Ultimately, it ensures scalable, consistent, and resilient access to the churn prediction system, empowering all teams to take timely, proactive retention actions."
      ],
      "metadata": {
        "id": "6-3an1MdFKbJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solution Approach"
      ],
      "metadata": {
        "id": "f85iiAH66Yog"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The crux of the solution approach is to ***decouple the frontend and backend of the app*** for better accessibility and allowing for easy integration with other services or systems.\n",
        "\n",
        "1. We’ll first build a Flask app (`app.py`) that loads the serialized ML model and exposes two API endpoints - one for single customer prediction and another for batch predictions using a CSV file.\n",
        "    - ***This backend handles the prediction logic*** and returns JSON responses.\n",
        "    - We’ll deploy this Flask app along with the model file and `requirements.txt` into a Hugging Face space using a `Dockerfile`, making it publicly accessible through a URL.\n",
        "\n",
        "2. Next, we’ll build a separate Streamlit app (`app.py`) that acts as the user interface. This app will include form inputs for online prediction and a CSV uploader for batch prediction.\n",
        "    - In both cases, it will ***send data to the Flask API*** using the `requests` library and display the responses in a clean, readable format.\n",
        "    - We’ll deploy this Streamlit app with its own `requirements.txt` to another Hugging Face space.\n",
        "\n",
        "3. Once both spaces are live, we’ll be able to access the model and make predictions for single as well as multiple users."
      ],
      "metadata": {
        "id": "vr1DasSw6bv-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# App Backend"
      ],
      "metadata": {
        "id": "QqdVZ2fkNylb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Points to note before executing the below cells\n",
        "- Go to **Hugging Face**\n",
        "- Open your **Profile**\n",
        "- Click on **New Space**\n",
        "  - Under the space creation, enter the below details\n",
        "    - Space name: **Backend**\n",
        "(If you were trying with different names, be cautious when using a underscore `_` in space names, such as `backend_space`, as it can cause exceptions when accessing the API URL. Always use hyphen `-` instead, like `backend-space`.)\n",
        "    - Select the space SDK: **Docker**\n",
        "    - Choose a Docker tempplate: **Blank**\n",
        "    - Click on **Create Space**"
      ],
      "metadata": {
        "id": "u13688-FmSlX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Flask Web Framework\n"
      ],
      "metadata": {
        "id": "T3XlDPUtJnDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a folder for storing the files needed for backend server deployment\n",
        "import os\n",
        "os.makedirs(\"backend_files\", exist_ok=True)"
      ],
      "metadata": {
        "id": "D4luHdaALN3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile backend_files/app.py\n",
        "import joblib\n",
        "import pandas as pd\n",
        "from flask import Flask, request, jsonify\n",
        "\n",
        "# Initialize Flask app with a name\n",
        "churn_predictor_api = Flask(\"Customer Churn Predictor\")\n",
        "\n",
        "# Load the trained churn prediction model\n",
        "model = joblib.load(\"churn_prediction_model_v1_0.joblib\")\n",
        "\n",
        "# Define a route for the home page\n",
        "@churn_predictor_api.get('/')\n",
        "def home():\n",
        "    return \"Welcome to the Customer Churn Prediction API!\"\n",
        "\n",
        "# Define an endpoint to predict churn for a single customer\n",
        "@churn_predictor_api.post('/v1/customer')\n",
        "def predict_churn():\n",
        "    # Get JSON data from the request\n",
        "    customer_data = request.get_json()\n",
        "\n",
        "    # Extract relevant customer features from the input data\n",
        "    sample = {\n",
        "        'CreditScore': customer_data['CreditScore'],\n",
        "        'Geography': customer_data['Geography'],\n",
        "        'Age': customer_data['Age'],\n",
        "        'Tenure': customer_data['Tenure'],\n",
        "        'Balance': customer_data['Balance'],\n",
        "        'NumOfProducts': customer_data['NumOfProducts'],\n",
        "        'HasCrCard': customer_data['HasCrCard'],\n",
        "        'IsActiveMember': customer_data['IsActiveMember'],\n",
        "        'EstimatedSalary': customer_data['EstimatedSalary']\n",
        "    }\n",
        "\n",
        "    # Convert the extracted data into a DataFrame\n",
        "    input_data = pd.DataFrame([sample])\n",
        "\n",
        "    # Make a churn prediction using the trained model\n",
        "    prediction = model.predict(input_data).tolist()[0]\n",
        "\n",
        "    # Map prediction result to a human-readable label\n",
        "    prediction_label = \"churn\" if prediction == 1 else \"not churn\"\n",
        "\n",
        "    # Return the prediction as a JSON response\n",
        "    return jsonify({'Prediction': prediction_label})\n",
        "\n",
        "# Define an endpoint to predict churn for a batch of customers\n",
        "@churn_predictor_api.post('/v1/customerbatch')\n",
        "def predict_churn_batch():\n",
        "    # Get the uploaded CSV file from the request\n",
        "    file = request.files['file']\n",
        "\n",
        "    # Read the file into a DataFrame\n",
        "    input_data = pd.read_csv(file)\n",
        "\n",
        "    # Make predictions for the batch data and convert raw predictions into a readable format\n",
        "    predictions = [\n",
        "        'Churn' if x == 1\n",
        "        else \"Not Churn\"\n",
        "        for x in model.predict(input_data.drop(\"CustomerId\",axis=1)).tolist()\n",
        "    ]\n",
        "\n",
        "    cust_id_list = input_data.CustomerId.values.tolist()\n",
        "    output_dict = dict(zip(cust_id_list, predictions))\n",
        "\n",
        "    return output_dict\n",
        "\n",
        "# Run the Flask app in debug mode\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwUlhcXQFJs4",
        "outputId": "c1549c39-2dcb-40cc-fbe2-b8508f3ed637"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing backend_files/app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies File"
      ],
      "metadata": {
        "id": "STDSb04iT-rL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile backend_files/requirements.txt\n",
        "pandas==2.2.2\n",
        "numpy==2.0.2\n",
        "scikit-learn==1.6.1\n",
        "xgboost==2.1.4\n",
        "joblib==1.4.2\n",
        "Werkzeug==2.2.2\n",
        "flask==2.2.2\n",
        "gunicorn==20.1.0\n",
        "requests==2.28.1\n",
        "uvicorn[standard]\n",
        "streamlit==1.43.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKj1zTp-FJm4",
        "outputId": "19feab36-e68d-46e1-b2a9-0f25d6370fa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing backend_files/requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dockerfile"
      ],
      "metadata": {
        "id": "JWD7rPCRUEtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile backend_files/Dockerfile\n",
        "FROM python:3.9-slim\n",
        "\n",
        "# Set the working directory inside the container\n",
        "WORKDIR /app\n",
        "\n",
        "# Copy all files from the current directory to the container's working directory\n",
        "COPY . .\n",
        "\n",
        "# Install dependencies from the requirements file without using cache to reduce image size\n",
        "RUN pip install --no-cache-dir --upgrade -r requirements.txt\n",
        "\n",
        "# Define the command to start the application using Gunicorn with 4 worker processes\n",
        "# - `-w 4`: Uses 4 worker processes for handling requests\n",
        "# - `-b 0.0.0.0:7860`: Binds the server to port 7860 on all network interfaces\n",
        "# - `app:app`: Runs the Flask app (assuming `app.py` contains the Flask instance named `app`)\n",
        "CMD [\"gunicorn\", \"-w\", \"4\", \"-b\", \"0.0.0.0:7860\", \"app:churn_predictor_api\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fujFE1-fFJkY",
        "outputId": "98993175-9194-49dc-ddd8-1e3c3bc35b06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing backend_files/Dockerfile\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Uploading Files to Hugging Face Space for the Backend"
      ],
      "metadata": {
        "id": "B4tnVrlo8xQ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**: Before running the code below, ensure that the serialized ML model has been uploaded in to `backend_files` folder."
      ],
      "metadata": {
        "id": "0thJ9hWq_aqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for hugging face space authentication to upload files\n",
        "from huggingface_hub import login, HfApi\n",
        "\n",
        "access_key = \"----------------\"  # Your Hugging Face token created from access keys in write mode\n",
        "repo_id = \"user/space_name\"  # Your Hugging Face space id\n",
        "\n",
        "# Login to Hugging Face platform with the access token\n",
        "login(token=access_key)\n",
        "\n",
        "# Initialize the API\n",
        "api = HfApi()\n",
        "\n",
        "# Upload Streamlit app files stored in the folder called deployment_files\n",
        "api.upload_folder(\n",
        "    folder_path=\"/content/backend_files\",  # Local folder path\n",
        "    repo_id=repo_id,  # Hugging face space id\n",
        "    repo_type=\"space\",  # Hugging face repo type \"space\"\n",
        ")"
      ],
      "metadata": {
        "id": "JqcGeow684xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# App Frontend"
      ],
      "metadata": {
        "id": "rz1AJNb4N8uG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Points to note before executing the below cells\n",
        "- Create a Streamlit space on Hugging Face by following the instructions provided on the content page titled **`Creating Spaces and Adding Secrets in Hugging Face`** from Week 1"
      ],
      "metadata": {
        "id": "2WxZ1IJ9m4Hv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Streamlit for Interactive UI"
      ],
      "metadata": {
        "id": "UsCYxkq_UL3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a folder for storing the files needed for frontend UI deployment\n",
        "os.makedirs(\"frontend_files\", exist_ok=True)"
      ],
      "metadata": {
        "id": "eOcW-xasOHZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile frontend_files/app.py\n",
        "import requests\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "\n",
        "st.title(\"Customer Churn Prediction\")\n",
        "\n",
        "# Batch Prediction\n",
        "st.subheader(\"Online Prediction\")\n",
        "\n",
        "# Input fields for customer data\n",
        "CustomerID = st.number_input(\"Customer ID\", min_value=10000000, max_value=99999999)\n",
        "CreditScore = st.number_input(\"Credit Score (customer's credit score)\", min_value=300, max_value=900, value=650)\n",
        "Geography = st.selectbox(\"Geography (country where the customer resides)\", [\"France\", \"Germany\", \"Spain\"])\n",
        "Age = st.number_input(\"Age (customer's age in years)\", min_value=18, max_value=100, value=30)\n",
        "Tenure = st.number_input(\"Tenure (number of years the customer has been with the bank)\", value=12)\n",
        "Balance = st.number_input(\"Account Balance (customer’s account balance)\", min_value=0.0, value=10000.0)\n",
        "NumOfProducts = st.number_input(\"Number of Products (number of products the customer has with the bank)\", min_value=1, value=1)\n",
        "HasCrCard = st.selectbox(\"Has Credit Card?\", [\"Yes\", \"No\"])\n",
        "IsActiveMember = st.selectbox(\"Is Active Member?\", [\"Yes\", \"No\"])\n",
        "EstimatedSalary = st.number_input(\"Estimated Salary (customer’s estimated salary)\", min_value=0.0, value=50000.0)\n",
        "\n",
        "customer_data = {\n",
        "    'CreditScore': CreditScore,\n",
        "    'Geography': Geography,\n",
        "    'Age': Age,\n",
        "    'Tenure': Tenure,\n",
        "    'Balance': Balance,\n",
        "    'NumOfProducts': NumOfProducts,\n",
        "    'HasCrCard': 1 if HasCrCard == \"Yes\" else 0,\n",
        "    'IsActiveMember': 1 if IsActiveMember == \"Yes\" else 0,\n",
        "    'EstimatedSalary': EstimatedSalary\n",
        "}\n",
        "\n",
        "if st.button(\"Predict\", type='primary'):\n",
        "    response = requests.post(\"https://<user_name>-<space_name>.hf.space/v1/customer\", json=customer_data)    # enter user name and space name before running the cell\n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        churn_prediction = result[\"Prediction\"]  # Extract only the value\n",
        "        st.write(f\"Based on the information provided, the customer with ID {CustomerID} is likely to {churn_prediction}.\")\n",
        "    else:\n",
        "        st.error(\"Error in API request\")\n",
        "\n",
        "# Batch Prediction\n",
        "st.subheader(\"Batch Prediction\")\n",
        "\n",
        "file = st.file_uploader(\"Upload CSV file\", type=[\"csv\"])\n",
        "if file is not None:\n",
        "    if st.button(\"Predict for Batch\", type='primary'):\n",
        "        response = requests.post(\"https://<user_name>-<space_name>.hf.space/v1/customerbatch\", files={\"file\": file})    # enter user name and space name before running the cell\n",
        "        if response.status_code == 200:\n",
        "            result = response.json()\n",
        "            st.header(\"Batch Prediction Results\")\n",
        "            st.write(result)\n",
        "        else:\n",
        "            st.error(\"Error in API request\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McWDBwZPFJe6",
        "outputId": "6dd6d574-41af-4438-a430-88f310cd5849"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing frontend_files/app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies File"
      ],
      "metadata": {
        "id": "beq1RbMhUQmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile frontend_files/requirements.txt\n",
        "pandas==2.2.2\n",
        "requests==2.28.1\n",
        "streamlit==1.43.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mRb5eXRR7u-",
        "outputId": "cc5832b1-13e4-4c75-a844-20e0e1977ba1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing frontend_files/app_requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dockerfile"
      ],
      "metadata": {
        "id": "h_SIZoHHMtqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile frontend_files/Dockerfile\n",
        "# Use a minimal base image with Python 3.9 installed\n",
        "FROM python:3.9-slim\n",
        "\n",
        "# Set the working directory inside the container to /app\n",
        "WORKDIR /app\n",
        "\n",
        "# Copy all files from the current directory on the host to the container's /app directory\n",
        "COPY . .\n",
        "\n",
        "# Install Python dependencies listed in requirements.txt\n",
        "RUN pip3 install -r requirements.txt\n",
        "\n",
        "# Define the command to run the Streamlit app on port 8501 and make it accessible externally\n",
        "CMD [\"streamlit\", \"run\", \"app.py\", \"--server.port=8501\", \"--server.address=0.0.0.0\", \"--server.enableXsrfProtection=false\"]\n",
        "\n",
        "# NOTE: Disable XSRF protection for easier external access in order to make batch predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzXGKRHbMgyO",
        "outputId": "8764009b-4fad-45c4-92db-069ac12f1e67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting frontend_files/Dockerfile\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Uploading Files to Hugging Face Space for the Frontend"
      ],
      "metadata": {
        "id": "5Re8ovwv9Rb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "access_key = \"-------------\"  # Your Hugging Face token created from access keys in write mode\n",
        "repo_id = \"user/space_name\"  # Your Hugging Face space id\n",
        "\n",
        "# Login to Hugging Face platform with the access token\n",
        "login(token=access_key)\n",
        "\n",
        "# Initialize the API\n",
        "api = HfApi()\n",
        "\n",
        "# Upload Streamlit app files stored in the folder called deployment_files\n",
        "api.upload_folder(\n",
        "    folder_path=\"/content/frontend_files\",  # Local folder path\n",
        "    repo_id=repo_id,  # Hugging face space id\n",
        "    repo_type=\"space\",  # Hugging face repo type \"space\"\n",
        ")"
      ],
      "metadata": {
        "id": "4b93nx2F9Qt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inferencing using Flask API\n"
      ],
      "metadata": {
        "id": "uL3mB7rziwnz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the ***frontend and backend are decoupled***, we can ***access the backend directly for predictions***.\n",
        "- The decoupling ensures seamless interaction with the deployed model while leveraging the API for scalable inference."
      ],
      "metadata": {
        "id": "P_m0lIv-qNIS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how to interact with the Flask API programatically within this notebook to perform **online** and **batch inference**.\n",
        "\n",
        "We will\n",
        "1. Send API requests for both online and batch inference.\n",
        "2. Process and check the model predictions."
      ],
      "metadata": {
        "id": "vXCEJGmzI_zS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9g9MT90dyLS"
      },
      "outputs": [],
      "source": [
        "import json  # To handle JSON formatting for API requests and responses\n",
        "import requests  # To send HTTP requests to the deployed Flask API\n",
        "\n",
        "import pandas as pd  # For data manipulation and analysis\n",
        "import numpy as np  # For numerical computations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_root_url = \"https://<user_name>-<space_name>.hf.space/\"  # Base URL of the deployed Flask API on Hugging Face Space; enter user name and space name before running the cell"
      ],
      "metadata": {
        "id": "jJx0262Ld6bD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_url = model_root_url + \"/v1/customer\"  # Endpoint for online (single) inference"
      ],
      "metadata": {
        "id": "Kq6LpmrjeITI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since our model predictions are served through the Flask endpoint we created, we need to call this endpoint to make a prediction.\n",
        "\n",
        "> ```@app.post('/v1/customer')```"
      ],
      "metadata": {
        "id": "l9oJ16-b1nE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_batch_url = model_root_url + \"/v1/customerbatch\"  # Endpoint for batch inference"
      ],
      "metadata": {
        "id": "0SzGv3INkDHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ```@app.post('/v1/customerbatch')```"
      ],
      "metadata": {
        "id": "wtg2VqmhkDst"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Online Inference"
      ],
      "metadata": {
        "id": "dPUTD0Snew7n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The idea is to send a single request to the API and receive an immediate response. This is useful for real-time applications like recommendation systems and fraud detection.\n",
        "\n",
        "* This data is sent as a JSON payload in a POST request to the model endpoint.\n",
        "* The model processes the input features and returns a prediction as a JSON payload."
      ],
      "metadata": {
        "id": "fhhivXJKFEfg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "payload = {\n",
        "    \"CreditScore\": 900,\n",
        "    \"Geography\": \"France\",\n",
        "    \"Age\": 67,\n",
        "    \"Tenure\": 1,\n",
        "    \"Balance\": 120000.50,\n",
        "    \"NumOfProducts\": 1,\n",
        "    \"HasCrCard\": 0,\n",
        "    \"IsActiveMember\": 0,\n",
        "    \"EstimatedSalary\": 95000.75\n",
        "}"
      ],
      "metadata": {
        "id": "hwBt4lwjeRIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sending a POST request to the model endpoint with the test payload\n",
        "response = requests.post(model_url, json=payload)"
      ],
      "metadata": {
        "id": "TEcUJhPueTg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_LRhw-wDPaQ",
        "outputId": "a4b66d75-d036-4f06-e4c2-8a9ebab3b916"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Response [200]>"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.json())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5siPq2UeVL7",
        "outputId": "51f8ef26-8994-4c11-e7f1-166243a196c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Prediction': 'churn'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch Inference"
      ],
      "metadata": {
        "id": "3mU64zPDeyKR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The idea is to send a batch of requests to the API and receive a response. The backend reads the entire dataset, runs it through the ML model, and returns the prediction for every row in the file. This is useful for applications like loan default prediction and customer churn prediction, where we don't need results instantaneously.\n",
        "\n",
        "* This data is sent as a CSV file in a POST request to the model endpoint.\n",
        "* The model processes each row containing the input features and returns the predictions for each row as one single JSON payload."
      ],
      "metadata": {
        "id": "aOEQQlw_FG6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "kzk8iEPBscR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "churn_dataset = pd.read_csv(\"batch_data.csv\")"
      ],
      "metadata": {
        "id": "S6TH6sbCscKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of numerical features in the dataset\n",
        "numeric_features = [\n",
        "    'CustomerId',        # Customer unique ID\n",
        "    'CreditScore',       # Customer's credit score\n",
        "    'Age',               # Customer's age\n",
        "    'Tenure',            # Number of years the customer has been with the bank\n",
        "    'Balance',           # Customer’s account balance\n",
        "    'NumOfProducts',     # Number of products the customer has with the bank\n",
        "    'HasCrCard',         # Whether the customer has a credit card (binary: 0 or 1)\n",
        "    'IsActiveMember',    # Whether the customer is an active member (binary: 0 or 1)\n",
        "    'EstimatedSalary'    # Customer’s estimated salary\n",
        "]\n",
        "\n",
        "# List of categorical features in the dataset\n",
        "categorical_features = [\n",
        "    'Geography',         # Country where the customer resides\n",
        "]\n",
        "\n",
        "# Define predictor matrix (X) using selected numeric and categorical features\n",
        "batch_input_data = churn_dataset[numeric_features + categorical_features]"
      ],
      "metadata": {
        "id": "IdPUvXEGeki5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare batch input for API request\n",
        "batch_input = {\n",
        "    'file': batch_input_data.to_csv(header=True, index=False).encode('utf-8')\n",
        "}"
      ],
      "metadata": {
        "id": "i6XXra-zfUGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Send request to the model API for batch predictions\n",
        "response = requests.post(\n",
        "    model_batch_url,  # Model endpoint URL\n",
        "    files=batch_input\n",
        ")"
      ],
      "metadata": {
        "id": "pWeW9HKafVxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnV65NCXBj6u",
        "outputId": "6daa90a7-16c1-47e2-e1fc-3cd96adfdaa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Response [200]>"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "JGVJ35iHChXq",
        "outputId": "4250a7ef-c7c9-4d10-fb50-e9b6a7cd914f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\"15574012\":\"Not Churn\",\"15592389\":\"Churn\",\"15592531\":\"Churn\",\"15619304\":\"Not Churn\",\"15656148\":\"Not Churn\",\"15701354\":\"Not Churn\",\"15737173\":\"Churn\",\"15737888\":\"Not Churn\",\"15767821\":\"Churn\",\"15792365\":\"Not Churn\"}\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- As we can see, we receive a JSON where each key represents a customer ID, and the value represents the model prediction of whether the customer will churn or not."
      ],
      "metadata": {
        "id": "gwwc6sK8sg1g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size=6 color=\"blue\">Power Ahead!</font>\n",
        "___"
      ],
      "metadata": {
        "id": "26px7tU1C02z"
      }
    }
  ]
}